version: '3.9'

services:
  # Production overrides for Next.js frontend
  web:
    build:
      target: prod
    environment:
      - NODE_ENV=production
      - NEXT_TELEMETRY_DISABLED=1
    command: ["npm", "start"]
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1024M
        reservations:
          cpus: '0.25'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Production overrides for orchestrator
  orchestrator:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=4
      - MAX_REQUESTS=1000
      - MAX_REQUESTS_JITTER=100
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2048M
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "app.main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8080", "--max-requests", "1000", "--max-requests-jitter", "100", "--preload"]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Production overrides for agents
  clarifier:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=2
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "a2a_server:app", "-w", "2", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:10001"]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  outline:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=2
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "a2a_server:app", "-w", "2", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:10002"]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  slide-writer:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=2
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1024M
        reservations:
          cpus: '0.2'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "a2a_server:app", "-w", "2", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:10003"]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  critic:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=1
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "a2a_server:app", "-w", "1", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:10004"]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  notes-polisher:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=1
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "a2a_server:app", "-w", "1", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:10005"]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  design:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=1
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "a2a_server:app", "-w", "1", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:10006"]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  script-writer:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=1
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "a2a_server:app", "-w", "1", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:10007"]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  research:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=2
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1024M
        reservations:
          cpus: '0.2'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "a2a_server:app", "-w", "2", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:10008"]
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  # Production overrides for MCP server
  mcp-server:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=2
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1024M
        reservations:
          cpus: '0.2'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: ["gunicorn", "main:app", "-w", "2", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8090"]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Production overrides for ArangoDB
  arangodb:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4096M
        reservations:
          cpus: '0.5'
          memory: 1024M
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
        window: 300s
    environment:
      - ARANGO_ROOT_PASSWORD_FILE=/run/secrets/arango_root_password
      - ARANGODB_OVERRIDE_DETECTED_TOTAL_MEMORY=4294967296
    secrets:
      - arango_root_password
    volumes:
      - arango-prod-data:/var/lib/arangodb3
      - arango-prod-apps:/var/lib/arangodb3-apps
      - arango-prod-logs:/var/log/arangodb3
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "5"
    command: >
      arangod
      --server.endpoint tcp://0.0.0.0:8529
      --database.directory /var/lib/arangodb3
      --log.output /var/log/arangodb3/arangod.log
      --log.level INFO
      --server.statistics true
      --server.maximal-queue-size 1048576

  # Production monitoring and observability
  prometheus:
    image: prom/prometheus:latest
    container_name: presentationpro-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2048M
        reservations:
          cpus: '0.2'
          memory: 512M
      restart_policy:
        condition: on-failure

  grafana:
    image: grafana/grafana:latest
    container_name: presentationpro-grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD_FILE=/run/secrets/grafana_admin_password
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - backend-network
    secrets:
      - grafana_admin_password
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
      restart_policy:
        condition: on-failure

  # Log aggregation
  loki:
    image: grafana/loki:latest
    container_name: presentationpro-loki
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki.yml:/etc/loki/local-config.yaml:ro
      - loki-data:/tmp/loki
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1024M
        reservations:
          cpus: '0.1'
          memory: 256M
      restart_policy:
        condition: on-failure

  # Reverse proxy and load balancer
  nginx:
    image: nginx:alpine
    container_name: presentationpro-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx-cache:/var/cache/nginx
      - nginx-logs:/var/log/nginx
    depends_on:
      - web
      - orchestrator
    networks:
      - frontend-network
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 64M
      restart_policy:
        condition: on-failure
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

secrets:
  arango_root_password:
    file: ./secrets/arango_root_password.txt
  grafana_admin_password:
    file: ./secrets/grafana_admin_password.txt

volumes:
  arango-prod-data:
    driver: local
  arango-prod-apps:
    driver: local
  arango-prod-logs:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  loki-data:
    driver: local
  nginx-cache:
    driver: local
  nginx-logs:
    driver: local